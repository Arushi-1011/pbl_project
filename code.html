<!DOCTYPE html>
<html>
<head>
    <title>Project Code - Smart Speech Therapy Assistant</title>
    <style>
        body {
            background-color: #0f172a;
            color: #f8fafc;
            font-family: monospace;
            padding: 40px;
        }
        h1 {
            color: #38bdf8;
        }
        pre {
            background-color: #1e293b;
            padding: 20px;
            overflow-x: auto;
            border-radius: 10px;
        }
    </style>
</head>
<body>

<h1>clean_dataset.py</h1>

<pre>
import os
import librosa
import numpy as np

INPUT_PATH = "dataset"
OUTPUT_PATH = "cleaned_dataset"

os.makedirs(OUTPUT_PATH, exist_ok=True)

SAMPLE_RATE = 16000
DURATION = 2  # seconds

def process_file(path):
    try:
        audio, sr = librosa.load(path, sr=SAMPLE_RATE)
        audio, _ = librosa.effects.trim(audio)
        
        if len(audio) > SAMPLE_RATE * DURATION:
            audio = audio[:SAMPLE_RATE * DURATION]
        else:
            padding = SAMPLE_RATE * DURATION - len(audio)
            audio = np.pad(audio, (0, padding))
            
        return audio
    except:
        return None

count = 0

for label_folder in ["non_dysarthric", "dysarthric"]:
    label = 0 if label_folder == "non_dysarthric" else 1
    
    folder_path = os.path.join(INPUT_PATH, label_folder)
    
    for speaker in os.listdir(folder_path):
        speaker_path = os.path.join(folder_path, speaker)
        
        for file in os.listdir(speaker_path):
            if file.endswith(".wav"):
                file_path = os.path.join(speaker_path, file)
                
                audio = process_file(file_path)
                
                if audio is not None:
                    np.save(
                        os.path.join(OUTPUT_PATH, f"{label}_{count}.npy"),
                        audio
                    )
                    count += 1

print("Cleaning complete")
print("Total processed files:", count)
</pre>

<hr>

<h1>train_model.py</h1>

<pre>
import os
import numpy as np
import librosa
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

data_root = "dataset"
sample_rate = 16000
max_duration = 2
max_length = sample_rate * max_duration

# Speaker-independent split
train_speakers = ["F01", "F03", "FC01", "FC02"]
test_speakers = ["F04", "FC03"]

X_train, y_train = [], []
X_test, y_test = [], []

print("Extracting features (Speaker-Independent Mode)...")

for label_name in os.listdir(data_root):
    label_path = os.path.join(data_root, label_name)

    if not os.path.isdir(label_path):
        continue

    # Assign label
    if label_name == "non_dysarthric":
        label = 0
    elif label_name == "dysarthric":
        label = 1
    else:
        continue

    for speaker in os.listdir(label_path):
        speaker_path = os.path.join(label_path, speaker)

        if not os.path.isdir(speaker_path):
            continue

        for file in os.listdir(speaker_path):
            if file.endswith(".wav"):
                file_path = os.path.join(speaker_path, file)

                try:
                    audio, sr = librosa.load(file_path, sr=sample_rate)

                    # Fix length
                    if len(audio) > max_length:
                        audio = audio[:max_length]
                    else:
                        audio = np.pad(audio, (0, max_length - len(audio)))

                    # Extract MFCC
                    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)
                    mfcc_mean = np.mean(mfcc.T, axis=0)

                    # Assign to train or test set
                    if speaker in train_speakers:
                        X_train.append(mfcc_mean)
                        y_train.append(label)
                    elif speaker in test_speakers:
                        X_test.append(mfcc_mean)
                        y_test.append(label)

                except Exception as e:
                    print(f"Error processing {file_path}: {e}")

print("Feature extraction complete.")
print("Train samples:", len(X_train))
print("Test samples:", len(X_test))

# Convert to numpy arrays
X_train = np.array(X_train)
y_train = np.array(y_train)
X_test = np.array(X_test)
y_test = np.array(y_test)

print("\nTraining model (Speaker-Independent)...")

model = RandomForestClassifier(
    n_estimators=200,
    random_state=42,
    class_weight="balanced"
)

model.fit(X_train, y_train)

y_pred = model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)

print("\nSpeaker-Independent Accuracy:", round(accuracy * 100, 2), "%")
print("\nClassification Report:\n")
print(classification_report(
    y_test,
    y_pred,
    target_names=["Non-Dysarthric", "Dysarthric"]
))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)

print("\nConfusion Matrix:")
print(cm)

plt.figure(figsize=(6,5))
sns.heatmap(
    cm,
    annot=True,
    fmt="d",
    cmap="Blues",
    xticklabels=["Non-Dysarthric", "Dysarthric"],
    yticklabels=["Non-Dysarthric", "Dysarthric"]
)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Speaker-Independent Confusion Matrix")
plt.tight_layout()
plt.show()
</pre>

<h1>Execution Output</h1>
<pre>
Extracting features...
Total samples: 2104

Class Distribution:
Non-dysarthric: 535
Dysarthric: 1569

Speaker-Independent Accuracy: 97.19 %

Confusion Matrix:
[[186  13]
 [  4 401]]
</pre>
    
<br><br>
<a href="index.html" style="color:#38bdf8;">Back to Presentation</a>

</body>
</html>
